"""
PES - Pandemic Experiment Scenario

Utility functions used throughout experiment
 • calculate_normalised_final_severity_performance_metric
 • chain_ops
 • confirm_biosemi_properly_initialised
 • create_random_subject_id
 • create_subject_id
 • generate_feedback
 • get_confidence_weighted_mean
 • get_confidence_weighted_median
 • get_confidence_weighted_mode
 • get_percent_deviation
 • get_sequence_severity_from_allocations
 • get_updated_severity
 • next_seq_length
 • random_severity_generator
 • remind_biosemi_properly_finalised
 • rgb_from_severity
 • sampler
"""


# ----------------
# external imports
# ----------------

import os
import glob
import numpy
import matplotlib.backends.backend_agg as agg
import matplotlib.pyplot as plt

from statsmodels.stats.weightstats import DescrStatsW as WeightedStats


# ----------------
# internal imports
# ----------------

from .. import ALLOCATION_TYPE
from .. import ANSI
from .. import BLOCK_MODE_INDICES
from .. import COLORS
from .. import FEEDBACK_SHOW_COMBINED_ALLOCATIONS
from .. import FEEDBACK_SHOW_GROUP_PERFORMANCE
from .. import FEEDBACK_SHOW_INDIVIDUAL_PERFORMANCES
from .. import INIT_NO_OF_CITIES
from .. import INPUTS_PATH
from .. import MAX_ALLOCATABLE_RESOURCES
from .. import MIN_ALLOCATABLE_RESOURCES
from .. import NUM_BLOCKS
from .. import NUM_SEQUENCES
from .. import OUTPUTS_PATH, OUTPUT_FILE_PREFIX
from .. import PLAYER_TYPE
from .. import RESPONSE_MULTIPLIER
from .. import SEVERITY_MULTIPLIER
from .. import SEQ_LENGTHS_FILE


# -----------------------
# module-global variables
# -----------------------

AllocationType                     = ALLOCATION_TYPE
FeedbackShowCombinedAllocations    = FEEDBACK_SHOW_COMBINED_ALLOCATIONS
FeedbackShowGroupPerformance       = FEEDBACK_SHOW_GROUP_PERFORMANCE
FeedbackShowIndividualPerformances = FEEDBACK_SHOW_INDIVIDUAL_PERFORMANCES


#############################
### General utility functions
#############################

def calculate_normalised_final_severity_performance_metric( SeveritiesFromSequence, InitialSequenceSeverities ):

    FinalSequenceSeverity     = numpy.sum( SeveritiesFromSequence )
    BestCaseAllocations       = numpy.full_like( SeveritiesFromSequence, MAX_ALLOCATABLE_RESOURCES )
    WorstCaseAllocations      = numpy.full_like( SeveritiesFromSequence, MIN_ALLOCATABLE_RESOURCES )
    BestCaseSequenceSeverity  = get_sequence_severity_from_allocations( BestCaseAllocations , InitialSequenceSeverities )
    WorstCaseSequenceSeverity = get_sequence_severity_from_allocations( WorstCaseAllocations, InitialSequenceSeverities )
    Performance               = (WorstCaseSequenceSeverity - FinalSequenceSeverity) / (WorstCaseSequenceSeverity - BestCaseSequenceSeverity )

    return Performance, WorstCaseSequenceSeverity, BestCaseSequenceSeverity




def get_sequence_severity_from_allocations( Allocations, InitialSeverities ):
    return numpy.sum( get_array_of_sequence_severities_from_allocations( Allocations, InitialSeverities ) )




def get_array_of_sequence_severities_from_allocations( Allocations, InitialSeverities ):
    '''
    This example shows how the city damage is performed in the current version of the experiment

    severities 3,4,8
    allocations 5,6,4

    3       |
    2.6 (5) |   4    |
    2.12(5) | 3.6 (6)|   8
    1.54(5) | 3.12(6)| 8.8 (4)

    Each individual severity is clipped to max(0, value)
    '''

    NumTrialsInSequence = len( InitialSeverities )
    severities          = []
    resources           = []

    for Trial in range( NumTrialsInSequence ):

        severities . append( InitialSeverities [ Trial ] )
        resources  . append( Allocations       [ Trial ] )

        severities = get_updated_severity( len( severities ), resources, severities )


    return severities.copy()




def create_random_subject_id() -> None :
    """
    Detects last subject number on the server, and increments by one, for use in tagging files generated by this
    experiment.
    """

    SubjectId  = chain_ops( random.randrange(1,99)
                            , lambda _ : f"{_:0>3}"              # convert back to string, zero-padded up to three digits
                          )

    return SubjectId




def create_subject_id() -> None :
    """
    Detects last subject number on the server, and increments by one, for use in tagging files generated by this
    experiment.
    """

    Prefix = f"{OUTPUT_FILE_PREFIX}log_"
    Suffix = ".txt"

    PrefixLength = len( Prefix )
    SuffixLength = len( Suffix )

    Subject_infoFiles = chain_ops( os.path.join( OUTPUTS_PATH, f'{Prefix}*{Suffix}' )
                                  , glob.glob
                                  , sorted
                                )
    Subject_infoFiles = [ os.path.basename( Str ) for Str in Subject_infoFiles ]

    SubjectIds = [ Str[ PrefixLength : -SuffixLength ] for Str in Subject_infoFiles ]   # keep only identifier part of files
    SubjectIds = [ Str[ : -5 ] if Str.endswith( '_TEST' ) else Str for Str in SubjectIds ]   # strip _TEST suffix if present
    SubjectIds = [ Str for Str in SubjectIds if Str.isdigit() ]                         # discard non numeric ids

    if SubjectIds == []: SubjectId = '001'
    else:
        SubjectId  = chain_ops( [ int(Str) for Str in SubjectIds ]   # reframe as numbers
                                , max                                # get the maximum
                                , lambda _ : _ + 1                   # increment by 1
                                , lambda _ : f"{_:0>3}"              # convert back to string, zero-padded up to three digits
                              )

    return SubjectId




def exit_experiment_gracefully( Message, Filehandles, MovementData, LogUtils, PygameMediator ):
    """
    Exit experiment gracefully, closing pygame, open files, and logging things appropriately.

    Note: We require the LogUtils and PygameMediator modules here as arguments, in order to avoid the cyclic dependency
    problem that would arise if we were to import them at the module level instead.
    """

  # Output helpful message - use these values in config to 'resume' a subsequent experiment
    LogUtils.tee()
    LogUtils.tee( Message )

  # Tidy up remaining resources
    numpy.save( *MovementData )
    PygameMediator.gracefully_quit_pygame()
    for Filehandle in Filehandles:
        if Filehandle is not None:   Filehandle.close()
    LogUtils.close_consolelog_filehandle()
    remind_biosemi_properly_finalised()




def chain_ops( Accumulant, *Functions_list ):
    """
    Chain Operations: Pipe an input into a series of functions, each of which receives as input the output of the
    previous one (i.e. like bash '|' pipes, or R's magrittr %>% operator, etc.
    """
    for f in Functions_list: Accumulant = f( Accumulant )
    return Accumulant




def rgb_from_severity( severity ):
    """
    Convert the severity to RGB value
    """

    if severity >= 10:   return COLORS[ 10 ]

    if severity > 0 :   rgb = COLORS[ int( severity ) ]
    else            :   rgb = COLORS[ 0               ]

    return rgb




def get_updated_severity( no_of_cities, resource_allocated, initial_severity ) -> [float]:
    """
    Update severity for existing cities given allocated resources as time progresses in the map.
    """

    UpdatedSeverity_list = []

    for c in range( no_of_cities ):

        InitialSeverityInCity    = initial_severity  [ c ]
        ResourcesAllocatedToCity = resource_allocated[ c ]
        NewSeverityInCity        = SEVERITY_MULTIPLIER * InitialSeverityInCity - RESPONSE_MULTIPLIER * ResourcesAllocatedToCity
        NewSeverityInCity        = max( NewSeverityInCity, 0 )

        UpdatedSeverity_list.append( NewSeverityInCity )


    return UpdatedSeverity_list




def sampler( samples, sum_to, range_list, rn = 100 ):
    """
    Obtain a number of random samples corresponding to some unknown criteria
    """

    # XXX My understanding here is that:
    #
    # - 'samples' here refers to the number of sequences in a block (typically initialised from 'NUM_SEQUENCES')
    #
    # - 'sum_to' is always set to 45, which I believe reflects the total number of trials that should exist in a block
    #   (i.e. over all sequences defined in that block). I have captured this number in the TOTAL_NUM_TRIALS_IN_BLOCK
    #   constant in the preamble of this module.
    #
    # - The intended use for the 'range_list' argument here is to contain the min and max number of trials that can be
    #   contained in a sequence (i.e. map)
    #
    # - 'rn' is an arbitrary number used as a seed. In practice it is passed the block number for which this function is
    #   called.
    #
    # - The sampler below works in two steps.
    #   1. In the first step, it says "obtain a random number of trials using a custom method, which involves getting
    #      the existing random slots containing numbers from 0 to num_of_sequences, and then come up with a new random
    #      number for each slot such this is the proportion of that random item in the slot over the sum of the whole
    #      array, and then multiplying by 45 (i.e. this is a method for obtaining a random number between 0 and 45), where
    #      45 represents the TOTAL_NUM_TRIALS_IN_BLOCK. If the number obtained is not between min_trials and max_trials,
    #      then discard this number and instead obtain a random number in that range normally using standard python
    #      facilities. It is not clear to me what advantage the custom method confers over standard python.
    #   2. In the second, while ensuring that entries are capped at min / max number of trials respectively, it either
    #      adds or subtracts from all number of trials randomly obtained in step 1, until a total of 45 trials is
    #      reached for the block

    assert range_list[ 0 ] < range_list[ 1 ], "Range should be a list, the first element of which is smaller than the second"

    numpy.random.seed( rn )

    arr = numpy.random.rand( samples )   # Arr represents a 'block', each entry represents a sequence, and the value of
                                         # each entry denotes the number of trials allocated randomly to that sequence
                                         # (subject to change below)

    sum_arr = sum( arr )   # The total number of trials in the block (we will later need to ensure this sums up to
                           # `sum_to`, i.e. 45)


  # Return a array, where values are first 'normalised' between 0 and `sum_to` (i.e. 45), and then 'validated' by only
  # keeping normalised values occurring between min_trials and max_trials, replacing invalid entries with a random
  # integer in that range.
    new_arr = numpy.array([
                          # (ternary operator syntax)
                                   int( item / sum_arr * sum_to )
                            if     (range_list[ 0 ] < int( item / sum_arr * sum_to ) < range_list[ 1 ])
                            else   numpy.random.choice( range( range_list[ 0 ], range_list[ 1 ] + 1 ) )

                          # within a list comprehension
                            for item in arr
                          ])

    difference = sum( new_arr ) - sum_to

  # Ensure that it is not possible for the while loop below to become infinite. This would occur in the following two
  # scenarios:
  #  1. The number of sequences in the block is so low,  that even if all sequences had the maximum number of trials, their sum would still always be below 45.
  #  2. The number of sequences in the block is so high, that even if all sequences had the minimum number of trials, their sum would still always be above 45.
  # E.g., for mintrials = 3 and maxtrials = 10, scenario 1 would occur for blocks having 4 sequences or fewer, and scenario 2 would occur for blocks having 16 sequences or higher.
  # Therefore the default selection of 8 sequences per block does not have this problem, but this may happen if this number is changed for testing purposes, so it is good to raise an exception here if this is the case.
  # (also see ticket:010)

    if len( samples ) * range_list[ 1 ] < sum_to   or   len( samples ) * range_list[ 0 ] > sum_to:
        raise ValueError( 'The specified number of sequences is such that the desired TOTAL_NUM_TRIALS_IN_BLOCK value can never be reached' )

    while difference != 0:

      # Sample indices (with replacement) as many times as needed to cover the difference, and increment or decrement at
      # those indices accordingly, to try to bring the difference up or down to 0 (i.e. such that the whole array sums
      # up to `sum_to` (i.e. 45)
        if difference < 0:
            for idx in numpy.random.choice( range( len( new_arr ) ), abs( difference ) ):
                if new_arr[ idx ] != range_list[ 1 ]:   new_arr[ idx ] += 1

        if difference > 0:
            for idx in numpy.random.choice( range( len( new_arr ) ), abs( difference ) ):
                if new_arr[ idx ] != 0 and new_arr[ idx ] != range_list[ 0 ]:   new_arr[ idx ] -= 1

        difference = sum( new_arr ) - sum_to


    return new_arr




def random_severity_generator( number_of_runs, lower_limit, upper_limit ):
    """
    Generate a random 'initial severity' value for each 'run' (i.e. city?)
    """

    x      = numpy.arange( lower_limit, upper_limit )
    xU, xL = x + 0.5, x - 0.5

    prob   = ss.norm.cdf( xU, scale = 100 ) - ss.norm.cdf( xL, scale = 100 )
    prob   = prob / prob.sum()  # normalize the probabilities so their sum is 1

    numpy.random.seed( 3 )

    nums = numpy.random.choice( x, size = number_of_runs, p = prob )


    return nums




def generate_feedback( SequenceIndices_1d_array, Performances, AllMessages, AggregatedAllocations, InitialSeveritiesInSequence ):
    """
    Create a plot object providing graphical feedback about the previous block.
    This is returned back to pygame which will handle when and how to display it.
    """

    global AllocationType
    global FeedbackShowCombinedAllocations
    global FeedbackShowGroupPerformance
    global FeedbackShowIndividualPerformances

  # Create two subplots: top for player allocations, bottom for group performance.
    Fig, Axes = plt.subplots( 2, 1, figsize = [7, 6], dpi = 100 )
    Fig.tight_layout( h_pad = 6, pad = 4 )

    # @FIXME Only ten players max allowed!
    colors = ['magenta', 'blue', 'yellow','green', 'orange', 'violet', 'skyblue', 'darkred', 'darkorange', 'darkgreen']

    NumAllocations  = len( InitialSeveritiesInSequence )
    NumPerformances = len( Performances )   # Note: also contains 'combined'
    NumPlayers      = len( AllMessages  )   # Note: does not contain 'combined'

    assert NumPlayers < 11, "ERROR: The experiment only supports a maximum of ten players"

  # Top graph: player allocations
    for i in range( NumPlayers ):
        PlayerLabel = 'You' if i == 0 else f'Player {i+1}'
        Axes[ 0 ].plot( [j + 0.1 * i for j in range( 1, NumAllocations + 1 )], AllMessages[ i ][ :, 0 ], 'o-', markersize = 10, color = colors[i], markeredgecolor = 'black', label = PlayerLabel )

    if FeedbackShowCombinedAllocations:
        Axes[ 0 ].plot( [j + 0.1 * NumPlayers for j in range( 1, NumAllocations + 1 )], AggregatedAllocations, 's-', markersize = 10, color = 'black', markeredgecolor = 'black', markerfacecolor = 'lightgray', label= f'Result of vote' )

    Axes[ 0 ].set_xlim( [ 0, NumAllocations + 1 ] )
    Axes[ 0 ].set_xticks( range( 1, NumAllocations + 1 ) )
    XTL = Axes[ 0 ].set_xticklabels( ['•'] * NumAllocations, fontsize = 80 )   # X Ticklabels: large bullet characters
                                                                               # indicating city markers: appropriate
                                                                               # colour indicating severity will be set
                                                                               # later

    Axes[ 0 ].set_ylim( [ MIN_ALLOCATABLE_RESOURCES - 1, MAX_ALLOCATABLE_RESOURCES + 1 ] )
    Axes[ 0 ].set_yticks( range( MIN_ALLOCATABLE_RESOURCES, MAX_ALLOCATABLE_RESOURCES + 1 ) )

    #Axes[ 0 ].set_xlabel( 'Sequence of Trials' )   # for some reason this does not appear - gets hidden by the second subplot ... therefore we cheat and create a text object instead.
    Axes[0].text( Axes[0].get_xlim()[1]/2-0.5, -5, 'Sequence of Trials' )

    Axes[ 0 ].grid( axis = 'y', color = 'k', ls = '--', lw = .5 )
    Axes[ 0 ].set_ylabel( 'Allocated resources' )
    Axes[ 0 ].set_title( 'Allocations by all players during the last sequence' )
    Axes[ 0 ].legend()

  # Change the colour of each city xtickmarker to reflect its severity
    for i in range( len( XTL ) ):
        XTL[ i ].set_color( tuple( j / 255. for j in rgb_from_severity( InitialSeveritiesInSequence[ i ] ) ) )
        XTL[ i ].set_position( (1, 0.15) )


    # -------------------------------
  # # Bottom graph: group performance
    # -------------------------------


  # Process plots for blocks individually
    PlayerLabels = [ 'You' if i == 0 else f'Player {i+1}' for i in range( NumPlayers ) ]
    GroupLabel   = 'Result of vote'

    for BlockGroup in split_1d_array_of_sequence_indices_into_blockgroups(
                        SequenceIndices_1d_array
                      ):

        BlockIndex = BlockGroup[ 0 ] // NUM_SEQUENCES

        if   BlockIndex in BLOCK_MODE_INDICES[ 'Joint' ]:   toggle_experiment_mode( 'Joint' )
        elif BlockIndex in BLOCK_MODE_INDICES[ 'Solo'  ]:   toggle_experiment_mode( 'Solo'  )
        else: raise ValueError( f"Current Block Index ({BlockIndex}) not registered in existing BLOCK_MODE_INDICES. Please correct this in the CONFIG." )

        if FeedbackShowIndividualPerformances:
            for i in range( NumPlayers ):

                Axes[ 1 ].plot(   (0.1 * i) + numpy.array( BlockGroup ),
                                  numpy.array( Performances[ i ] )[ BlockGroup ],
                                  's-',
                                  markersize = 10,
                                  color = colors[ i ],
                                  markeredgecolor = 'black',
                                  label = PlayerLabels[ i ]   )

                PlayerLabels[ i ] = None   # only use label for a single block


        if FeedbackShowGroupPerformance or not FeedbackShowIndividualPerformances:
            Axes[ 1 ].plot(  0.1 * len( SequenceIndices_1d_array ) + numpy.array( BlockGroup ),
                             numpy.array( Performances[ -1 ] )[ BlockGroup ],
                             'ks-',
                             linewidth = 2,
                             markersize = 12,
                             markeredgecolor = 'black',
                             markerfacecolor = 'lightgray',
                             label = GroupLabel   )

            GroupLabel = None   # only use a label for the plot corresponding to the first group plot (i.e. avoid repeating the label for remaining blocks).


  # Finalise bottom graph
    Axes[ 1 ].set_xlabel( 'Number of Sequences' )
    Axes[ 1 ].set_ylabel( 'Performance' )

    xmax = 20
    xstep = 1
    while SequenceIndices_1d_array[ -1 ] > xmax:
        xmax += 20
        if   xmax < 50 : xstep = 2
        elif xmax < 100: xstep = 5
        else           : xstep = 10

    Axes[ 1 ].set_xlim( -1, xmax )
    Axes[ 1 ].set_ylim( -0.1, 1.1  )

    Axes[ 1 ].set_xticks(      (numpy.arange( 0, xmax + 1, xstep ) - 1)[ 1 : ] )                  # XXX: "Clever" code. We place the label "x" at tick "x-1"
    Axes[ 1 ].set_xticklabels( (numpy.arange( 0, xmax + 1, xstep )    )[ 1 : ], rotation = 90 )   #      This effectively converts zero-index to one-index ordinal labels.

    Axes[ 1 ].set_yticks(      [      0,  0.25,  0.5, 0.75,     1] )
    Axes[ 1 ].set_yticklabels( ['Worst', '25%','50%','75%','Best'] )
    Axes[ 1 ].grid( axis = 'y', color = 'k', ls = '--', lw = .5 )

    Axes[ 1 ].set_title('Performance (of group vote or of individuals as appropriate)')

    Axes[ 1 ].legend()

    canvas = agg.FigureCanvasAgg( Fig )

    canvas.draw()

    renderer = canvas.get_renderer()
    raw_data = renderer.tostring_rgb()

    plt.close()


    return canvas, raw_data




def split_1d_array_of_sequence_indices_into_blockgroups( S ):

  # Get a list of indices corresponding to the first sequence index in each block
    FirstIndices = [i for i in S if i % NUM_SEQUENCES == 0]

  # Populate the desired array
    In    = S.tolist()
    Out   = []
    Block = []

    Block.append( In.pop( 0 ) )   # move the leading element of In to the end of
                                  # (the current) Block

    while len( In ) > 0:
        if In[ 0 ] in FirstIndices:
            Out.append( Block )
            Block = []

        Block.append( In.pop( 0 ) )

    Out.append( Block )


    return Out




def next_seq_length(index, seq_per_block):
    """
    Based on current global seq index (0-359), retrieves the next 'seq_per_block' sequence lengths
    """
    SequenceLengthsCsv = os.path.join( INPUTS_PATH, SEQ_LENGTHS_FILE )
    s = numpy.loadtxt( SequenceLengthsCsv, delimiter=',' )
    sequence = s[ index : index + seq_per_block ]
    return sequence




def get_confidence_weighted_mean( all_messages, first_severity, AbsoluteSequenceIndex, AbsoluteTrialCount ):

    # First let's get the aggregated allocations
    NumTrials = numpy.shape( all_messages )[1]
    AggregatedAllocations = []

    for t in range( NumTrials ):

      # Get only valid responses and confidences for this trial
        TrialResponses   = numpy.array( all_messages )[ :, t, 0 ]
        TrialConfidences = numpy.array( all_messages )[ :, t, 1 ]

        OriginalTrialResponses = TrialResponses.copy()

        TrialResponses   = TrialResponses   [ TrialConfidences != -1 ]
        TrialConfidences = TrialConfidences [ TrialConfidences != -1 ]

      # TrialConfidences cannot be negative (-1 are already excluded)
      # So if the sum is zero, this means that all the values are zero
      # so they cannot be used as weight in numpy.average !
      # Let's all weigth one, and count in the same way (Ticket:085)
        if (numpy.sum( TrialConfidences ) == 0):
            TrialConfidences[:] = 1.0

      # In the unlikely case of no valid confidences, set the allocations to the plain mean of all participants
        if numpy.size( TrialConfidences ) == 0:
            ConfidenceWeightedMean = numpy.mean( OriginalTrialResponses )
        else:
            ConfidenceWeightedMean = numpy.average( TrialResponses, weights = TrialConfidences )

        AggregatedAllocations.append( ConfidenceWeightedMean )

    AggregatedAllocations = numpy.array( AggregatedAllocations )
    AggregatedAllocations = numpy.round( AggregatedAllocations )

  # Second, let's get the theoretical severity for that aggregate
    SeverityFromAggregate = get_array_of_sequence_severities_from_allocations( AggregatedAllocations, first_severity[ AbsoluteTrialCount - NumTrials : AbsoluteTrialCount ].copy() )


    return AggregatedAllocations, SeverityFromAggregate




def get_confidence_weighted_mode():   raise NotImplementedError




def get_confidence_weighted_median( all_messages, first_severity,  AbsoluteSequenceIndex, AbsoluteTrialCount ):

  # First let's get the aggregated allocations
    NumTrials = numpy.shape( all_messages )[1]
    AggregatedAllocations = []

    for t in range( NumTrials ):

      # Get only valid responses and confidences for this trial
        TrialResponses   = numpy.array( all_messages )[ :, t, 0 ]
        TrialConfidences = numpy.array( all_messages )[ :, t, 1 ]

        OriginalTrialResponses = TrialResponses.copy()

        TrialResponses   = TrialResponses   [ TrialConfidences != -1 ]
        TrialConfidences = TrialConfidences [ TrialConfidences != -1 ]

      # In the unlikely case of no valid confidences, set the allocations to the plain median of all participants
        if numpy.size( TrialConfidences ) == 0:
            ConfidenceWeightedMedian = numpy.median( OriginalTrialResponses )
        else:

          # If only one valid response, duplicate for weightedmedian to work
            if numpy.size( TrialResponses ) == 1:
                TrialResponses   = numpy.repeat( TrialResponses  , 2 )
                TrialConfidences = numpy.repeat( TrialConfidences, 2 )

            ConfidenceWeightedMedian = WeightedStats(
                data    = TrialResponses,
                weights = TrialConfidences
            ).quantile(
                probs = [0.5],
                return_pandas = False
            )[0]

        AggregatedAllocations.append( ConfidenceWeightedMedian )

    AggregatedAllocations = numpy.array( AggregatedAllocations )
    AggregatedAllocations = numpy.round( AggregatedAllocations )

  # Second, let's get the theoretical severity for that aggregate
    SeverityFromAggregate = get_array_of_sequence_severities_from_allocations( AggregatedAllocations, first_severity[ AbsoluteTrialCount - NumTrials : AbsoluteTrialCount ].copy() )


    return AggregatedAllocations, SeverityFromAggregate




def get_percent_deviation( TrialSeverities, opt_final_severity):
    '''
    Get the percent deviation of the final severities for the sequence in 'TrialSeverities' from the optimal final
    severities in opt_final_severity.
    '''
    final_severity              = numpy.sum( TrialSeverities )
    OptimalTotalSeverityForSeq  = numpy.sum( opt_final_severity )
    DifferenceInSeverities      = numpy.abs ( OptimalTotalSeverityForSeq - final_severity )   # NEEDS TO CHANGE see ticket:027
    CompatibilityWithOptimal    = 100 - (DifferenceInSeverities / OptimalTotalSeverityForSeq * 100.)
    CompatibilityWithOptimal    = max( 0, CompatibilityWithOptimal ) # clip negative values to 0
    return CompatibilityWithOptimal




def confirm_biosemi_properly_initialised( MySubjectId ):
    RandomWords = [ 'curtain', 'belief', 'dependent', 'rice', 'trains', 'intelligent', 'charming', 'road', 'proud', 'scarf', 'unknown', 'broken', 'migrate', 'cuddly', 'induce', 'knock', 'motion', 'exclude', 'dispose', 'lift' ]

    print( f"This Subject's id is: {MySubjectId}" )
    print()
    print( "Please created an appropriately named file in the 'biosemidata' directory for this subject." )

    RandomWord = RandomWords[ numpy.random.randint(20) ]

    print( f"When you have done this, please type the following word below to proceed, exactly as it appears here: {ANSI.ORANGE}{RandomWord}{ANSI.RESET}" )
    print()

    InputResponse = input( "Type the word here to confirm: " )

    while( True ):
        if InputResponse == RandomWord:   break
        else:    InputResponse = input( "Wrong word ... please try again: " )

    print()
    print( f"Now, tell the participant to {ANSI.RED}CLOSE THEIR EYES FOR 30 seconds{ANSI.RESET}, while you UNPAUSE Actiview to actually start the recording!" )

    RandomWord = RandomWords[ numpy.random.randint(20) ]

    print( f"Once you've done this, please type the following word below to proceed, exactly as it appears here: {ANSI.ORANGE}{RandomWord}{ANSI.RESET}" )
    print()

    InputResponse = input( "Type the word here to confirm: " )

    while( True ):
        if InputResponse == RandomWord:   break
        else:    InputResponse = input( "Wrong word ... please try again: " )

    print( "Recording setup confirmed by experimenter. Proceeding..." )
    print()




def remind_biosemi_properly_finalised():
    print()
    print(f"{ANSI.RED}Note to Experimenter: Please remember to pause and save Actiview before you start taking off electrodes!{ANSI.RESET}")
    print()



def get_actual_allocation_given_allocation_type( AggregatedAllocations, MyMessage, AllMessages ):

    global AllocationType

    MyConfidence        = MyMessage[ -1, 1 ] if MyMessage[ -1, 1 ] != -1 else 0
    SumOfAllConfidences = sum( Msg[ -1, 1 ] if Msg[ -1, 1 ] != -1 else 0 for Msg in AllMessages)
    ConfidenceRatio     = (MyConfidence / SumOfAllConfidences) if SumOfAllConfidences > 0 else 0

    allocated_resources_given_shared_allocation_type       = s = int( AggregatedAllocations[ -1 ] )
    allocated_resources_given_individual_allocation_type       = int( MyMessage[ -1, 0 ] )
    allocated_resources_given_penalised_allocation_type        = s
    allocated_resources_given_proportional_allocation_type     = s

    if   AllocationType == 'shared'      :   AllocatedResources = allocated_resources_given_shared_allocation_type
    elif AllocationType == 'individual'  :   AllocatedResources = allocated_resources_given_individual_allocation_type
    elif AllocationType == 'penalised'   :   AllocatedResources = allocated_resources_given_penalised_allocation_type
    elif AllocationType == 'proportional':   AllocatedResources = allocated_resources_given_proportional_allocation_type
    else: raise ValueError( f"Invalid value detected for variable AllocationType: '{AllocationType}'" )


    return AllocatedResources




def get_resources_left_given_allocation_type( CurrentResourcesLeft, AggregatedAllocations, MyMessage, AllMessages ):

    global AllocationType

    MyConfidence        = MyMessage[ -1, 1 ] if MyMessage[ -1, 1 ] != -1 else 0
    SumOfAllConfidences = sum( Msg[ -1, 1 ] if Msg[ -1, 1 ] != -1 else 0 for Msg in AllMessages)
    ConfidenceRatio     = (MyConfidence / SumOfAllConfidences) if SumOfAllConfidences > 0 else 0

    resources_left_given_shared_allocation_type       = s = int( CurrentResourcesLeft - AggregatedAllocations[-1] )
    resources_left_given_individual_allocation_type   = i = int( CurrentResourcesLeft - MyMessage[ -1, 0 ]        )
    resources_left_given_penalised_allocation_type        = min( s, i )
    resources_left_given_proportional_allocation_type     = round( CurrentResourcesLeft - AggregatedAllocations[ -1 ] * ConfidenceRatio )


    if   AllocationType == 'shared'      :   ResourcesLeft = resources_left_given_shared_allocation_type
    elif AllocationType == 'individual'  :   ResourcesLeft = resources_left_given_individual_allocation_type
    elif AllocationType == 'penalised'   :   ResourcesLeft = resources_left_given_penalised_allocation_type
    elif AllocationType == 'proportional':   ResourcesLeft = resources_left_given_proportional_allocation_type
    else: raise ValueError( f"Invalid value detected for variable AllocationType: '{AllocationType}'" )


    return ResourcesLeft




def toggle_experiment_mode( Mode = 'Joint' ):

    global AllocationType
    global FeedbackShowCombinedAllocations
    global FeedbackShowGroupPerformance
    global FeedbackShowIndividualPerformances

    if Mode == 'Joint':
        AllocationType                     = 'shared'
        FeedbackShowCombinedAllocations    = True
        FeedbackShowGroupPerformance       = True
        FeedbackShowIndividualPerformances = False
    elif Mode == 'Solo':
        AllocationType                     = 'individual'
        FeedbackShowCombinedAllocations    = False
        FeedbackShowGroupPerformance       = False
        FeedbackShowIndividualPerformances = True
    else: raise ValueError( f"Invalid experiment mode given: {Mode}" )
